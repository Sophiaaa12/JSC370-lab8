---
title: "Lab 08 - Text Mining/NLP"
output: html_document
---

# Learning goals

- Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text
- Use dplyr and ggplot2 to analyze and visualize text data
- Try a theme model using `topicmodels`

# Lab description

For this lab we will be working with the medical record transcriptions from https://www.mtsamples.com/ available at https://github.com/JSC370/JSC370-2025/tree/main/data/medical_transcriptions.

# Deliverables

1. Questions 1-7 answered, knit to pdf or html output uploaded to Quercus.

2. Render the Rmarkdown document using `github_document` and add it to your github site. Add link to github site in your html.


### Setup packages

You should load in `tidyverse`, (or `data.table`), `tidytext`, `wordcloud2`, `tm`, and `topicmodels`.


## Read in the Medical Transcriptions

Loading in reference transcription samples from https://www.mtsamples.com/

```{r}
library(tidytext)
library(tidyverse)
library(wordcloud2)
library(tm)
library(topicmodels)

mt_samples <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2025/main/data/medical_transcriptions/mtsamples.csv")
mt_samples <- mt_samples |>
  select(description, medical_specialty, transcription)

head(mt_samples)
```

---

## Question 1: What specialties do we have?

We can use `count()` from `dplyr` to figure out how many different medical specialties are in the data. Are these categories related? overlapping? evenly distributed? Make a bar plot.

```{r}
mt_samples |>
  count(medical_specialty, sort = TRUE) |>
  ggplot(aes(x = reorder(medical_specialty, n), y = n)) + 
  geom_bar(stat = "identity") +
  coord_flip() + 
  labs(x = "Medical Specialty", y = "Count", title = "Distribution of Medical Specialties") +
  theme_minimal()
```

---

## Question 2: Tokenize

- Tokenize the the words in the `transcription` column
- Count the number of times each token appears
- Visualize the top 20 most frequent words with a bar plot
- Create a word cloud of the top 20 most frequent words

### Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r}
tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  count(word, sort = TRUE)

tokens |>
  slice_max(n, n = 20) |>  
  ggplot(aes(x = reorder(word, n), y = n)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  coord_flip() +
  labs(title = "Top 20 Most Frequent Words", x = "Words", y = "Frequency") +
  theme_minimal()

tokens |> 
  slice_max(n, n = 20) |> 
  wordcloud2(size = 1)
```
From the results shown in the bar plot and word cloud, we observe that the most frequent words in the transcription column are common stopwords such as "the," "and," "was," "of," and so on. It makes sense because the stopwords are usually very common and appear frequently within the documents.However, these words do not carry meaningful content for analysis. To gain more insights, we should remove stopwords so that we may know more information related to medical things through the barplot. Also, word "patient" appears among the frequent words, suggesting that they often reference patients directly.
---

## Question 3: Stopwords

- Redo Question 2 but remove stopwords
- Check `stopwords()` library and `stop_words` in `tidytext`
- Use regex to remove numbers as well
- Try customizing your stopwords list to include 3-4 additional words that do not appear informative

### What do we see when you remove stopwords and then when you filter further? Does it give us a better idea of what the text is about?

```{r}
head(stopwords("english"))
length(stopwords("english"))
head(stop_words)

tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |> 
  filter(!word %in% stop_words$word) |>  
  filter(!str_detect(word, "^[0-9]+$")) |>  #
  count(word, sort = TRUE)

tokens |>
  slice_max(n, n = 20) |>  
  ggplot(aes(x = reorder(word, n), y = n)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  coord_flip() +
  labs(title = "Top 20 Most Frequent Words (Removing Stopwords)", x = "Words", y = "Frequency") +
  theme_minimal()

tokens |> 
  slice_max(n, n = 20) |> 
  wordcloud2(size = 0.6)
```
- After removing standard stopwords, the results now highlight more domain-specific terms, providing a better idea of what the text is about. For example, "patient," "procedure," "history", "pain", "anesthesia", and so on suggest that the dataset is primarily medical, likely containing clinical notes, surgical records, or treatment summaries. Additionally, words like "mg" and "mm" may relate to dosage measurements or anatomical references.

---

## Question 4: ngrams

Repeat question 2, but this time tokenize into bi-grams. How does the result change if you look at tri-grams? Note we need to remove stopwords a little differently. You don't need to recreate the wordclouds.

```{r}
stopwords2 <- c(stopwords("en"), "mm", "mg", "noted")
sw_start <- paste0("^", paste(stopwords2, collapse=" |^"), "$")
sw_end <- paste0("", paste(stopwords2, collapse="$| "), "$")

# Bi-grams
tokens_bigram <- mt_samples |>
  select(transcription) |>
  unnest_tokens(ngram, transcription, token = "ngrams", n = 2) |>
  separate(ngram, into = c("word1", "word2"), sep = " ") |>  
  filter(!word1 %in% stopwords2, !word2 %in% stopwords2) |> 
  unite(bigram, word1, word2, sep = " ") |>  
  count(bigram, sort = TRUE)
  
tokens_bigram |> 
  slice_max(n, n = 20) |>  
  ggplot(aes(x = reorder(bigram, n), y = n)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  coord_flip() +
  labs(title = "Top 20 Most Frequent Bigrams", x = "Bigrams", y = "Frequency") +
  theme_minimal()
```
```{r}
# Tri-grams
tokens_trigram <- mt_samples |>
  select(transcription) |>
  unnest_tokens(ngram, transcription, token = "ngrams", n = 3) |>
  separate(ngram, into = c("word1", "word2", "word3"), sep = " ") |>  
  filter(!word1 %in% stopwords2, !word2 %in% stopwords2, !word3 %in% stopwords2) |> 
  unite(trigram, word1, word2, word3, sep = " ") |>  
  count(trigram, sort = TRUE)
  
tokens_trigram |> 
  slice_max(n, n = 20) |>  
  ggplot(aes(x = reorder(trigram, n), y = n)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  coord_flip() +
  labs(title = "Top 20 Most Frequent Trigrams", x = "Trigrams", y = "Frequency") +
  theme_minimal()
```
- Bigrams highlight key medical terms like "year old", "operating room", "preoperative diagnosis" and etc capturing patient demographics and procedural details. Trigrams provide more context and more information because it is with phrases like "estimated blood loss", "past medical history" and etc offering deeper insights into patient assessments, procedures, and surgical details. Compared to bigrams, trigrams enhance specificity, so that trigrams can contain more information to make us know details.

---

## Question 5: Examining words

Using the results from the bigram, pick a word and count the words that appear before and after it, and create a plot of the top 20.

```{r}
library(stringr)
target_word <- "preoperative"

# Extract and count words before/after the target
associated_words <- tokens_bigram %>%
  separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  filter(word1 == target_word | word2 == target_word) %>%
  mutate(neighbor = ifelse(word1 == target_word, word2, word1)) %>%
  count(neighbor, wt = n, sort = TRUE) %>%
  slice_max(n, n = 20)

# Generate the plot
ggplot(associated_words, aes(x = reorder(neighbor, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(
    title = paste("Top 20 Words Associated with '", target_word, "'"),
    x = "Neighboring Words",
    y = "Frequency"
  ) +
  theme_minimal()
```

---


## Question 6: Words by Specialties

Which words are most used in each of the specialties? You can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the 5 most used words?


```{r}
mt_samples |>
  unnest_tokens(word, transcription) |>
  filter(!word %in% stopwords2) |>    
  group_by(medical_specialty) |>   
  count(word, sort = TRUE) |>          
  top_n(5, n) |>                   
  ungroup()                     
```
- The 5 most used words are all in Surgery medical_specialty, suggesting that surgical reports contribute significantly to the dataset.

## Question 7: Topic Models

See if there are any themes in the data by using a topic model (LDA). 

- you first need to create a document term matrix
- then you can try the LDA function in `topicmodels`. Try different k values.
- create a facet plot of the results from the LDA (see code from lecture)


```{r}
library(reshape2)
# Create Document-Term Matrix
transcripts_dtm <- mt_samples |>
  mutate(document = row_number()) |>  
  select(document, transcription) |>
  unnest_tokens(word, transcription) |>
  filter(!word %in% stopwords2) |>   
  filter(nchar(word) > 2) |>        
  count(document, word) |>        
  cast_dtm(document, word, n)    


transcripts_lda <- LDA(transcripts_dtm, 
                      k = 4,     
                      control = list(seed = 1234))

transcripts_dtm <- as.matrix(transcripts_dtm)   

lda_topics <- tidy(transcripts_lda, matrix = "beta")

top_terms <- lda_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ggplot(top_terms, aes(x = reorder_within(term, beta, topic), 
                     y = beta, 
                     fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 2) +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = "Term Probability (beta)") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 8))

```




